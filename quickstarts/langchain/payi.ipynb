{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Payi client and limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from payi import Payi\n",
    "import os \n",
    "\n",
    "#Read the API KEYs from the environment, replace the default values (the second argument) with your own keys if needed\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_KEY\")\n",
    "payi_api_key = os.getenv(\"PAYI_API_KEY\", \"YOUR_PAYI_API_KEY\")\n",
    "\n",
    "payi_client = Payi(\n",
    "    api_key=payi_api_key\n",
    ")\n",
    "\n",
    "#Create a limit\n",
    "limit_response = payi_client.limits.create(\n",
    "    #As long as the limit configuration remains the same across creates, the same limit name can be used repeatedly\n",
    "    limit_name='Langchain quickstart allow limit', \n",
    "    max=12.50, #$12.50 USD\n",
    "    limit_type=\"Allow\",\n",
    "    limit_tags=[\"example_limit\"]\n",
    ")\n",
    "\n",
    "limit_name = limit_response.limit.limit_name\n",
    "limit_id = limit_response.limit.limit_id\n",
    "\n",
    "print(\"Limit Created\")\n",
    "print(f\"Limit Name: {limit_name}\")\n",
    "print(f\"Limit ID: {limit_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the callback to be invoked when the LLM call ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class PayiHandler(BaseCallbackHandler):\n",
    "    def __init__(self, client, params):\n",
    "        self.name = \"custom_handler\"\n",
    "        self.client = client\n",
    "        self.params = {\n",
    "            **params\n",
    "        }\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        llm_output = response.llm_output\n",
    "        if llm_output and 'token_usage' in llm_output:\n",
    "            token_usage = llm_output['token_usage']\n",
    "            prompt_tokens = token_usage.get('prompt_tokens', 0)\n",
    "            completion_tokens = token_usage.get('completion_tokens', 0)\n",
    "\n",
    "            if not (prompt_tokens > 0 or completion_tokens > 0):\n",
    "                print(f\"{self.name}: no token usage in LLM output\", response)\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                # comment either limit_ids or request_tags if you don't want to use them\n",
    "                result = self.client.ingest.units(\n",
    "                    category=self.params['category'],\n",
    "                    resource=self.params['resource'],\n",
    "                    units={ \"text\": { \"input\": prompt_tokens, \"output\": completion_tokens} },\n",
    "                    limit_ids=self.params['limit_ids'], \n",
    "                    request_tags=self.params['request_tags']\n",
    "                    )\n",
    "                print(f'ingest result: {result.model_dump_json(indent=4)}')\n",
    "            except Exception as e:\n",
    "                print(f\"{self.name}: error sending usage info\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the parameters for the call to OpenAI and ingesting the token counts into Payi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from payi.lib.helpers import PayiCategories\n",
    "\n",
    "# Configuration parameters for the Pay-i API\n",
    "params = {\n",
    "    'category': PayiCategories.openai,\n",
    "    'resource': 'gpt-3.5-turbo',\n",
    "    'limit_ids': [limit_id],  \n",
    "    'request_tags': ['x', 'y']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an OpenAI request and register the callback handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"say this only: hi\"])\n",
    "\n",
    "# Define the handler\n",
    "handler = PayiHandler(client=payi_client, params=params)\n",
    "\n",
    "# Define the LLM and register the handler \n",
    "model = ChatOpenAI( \n",
    "    model=params['resource'],\n",
    "    api_key=openai_key,\n",
    "    callbacks=[handler]\n",
    "    )\n",
    "\n",
    "# Define the sequence\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the sequence\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response.to_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
